{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from peft import LoraConfig, LoraModel, get_peft_model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load model and tokenizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_checkpoint = 'EleutherAI/gpt-neo-125M'\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n", "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n", "tokenizer.pad_token = tokenizer.eos_token"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define lambda regularization parameter as per paper details"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lambda_reg = 0.1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Placeholder for the dataset loading function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = [{\"prompt\": \"Example prompt\", \"response\": \"Example response\"}]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define LoRA configuration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lora_config = LoraConfig(\n", "    r=128,  # rank of LoRA\n", "    lora_alpha=256,  # scaling factor for initialization\n", "    lora_dropout=0.05,\n", "    bias=\"none\",\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wrap the model with LoRA layers for parameter-efficient training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["peft_model = get_peft_model(model, lora_config)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the compute_spin_loss function (with expected tensor shapes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compute_spin_loss(model_logits_gt, opponent_logits_gt, model_logits_syn, opponent_logits_syn, ground_truth_ids, synthetic_response_ids, lambda_reg):\n", "    # Apply softmax to convert logits to probabilities\n", "    # Shapes after softmax: [batch_size, sequence_length, vocab_size]\n", "    model_probs_gt = torch.nn.functional.softmax(model_logits_gt, dim=-1)\n", "    opponent_probs_gt = torch.nn.functional.softmax(opponent_logits_gt, dim=-1)\n", "    model_probs_syn = torch.nn.functional.softmax(model_logits_syn, dim=-1)\n", "    opponent_probs_syn = torch.nn.functional.softmax(opponent_logits_syn, dim=-1)\n\n", "    # Gather log probabilities for the actual tokens in the ground truth sequence\n", "    # [batch_size, sequence_length, vocab_size] -> [batch_size, sequence_length]\n", "    log_model_probs_gt = torch.log(torch.gather(\n", "        model_probs_gt, dim=2, index=ground_truth_ids.unsqueeze(-1)\n", "    ).squeeze(-1))\n", "    log_opponent_probs_gt = torch.log(torch.gather(\n", "        opponent_probs_gt, dim=2, index=ground_truth_ids.unsqueeze(-1)\n", "    ).squeeze(-1))\n\n", "    # Gather log probabilities for the actual tokens in the synthetic sequence\n", "    # [batch_size, sequence_length, vocab_size] -> [batch_size, sequence_length]\n", "    log_model_probs_syn = torch.log(torch.gather(\n", "        model_probs_syn, dim=2, index=synthetic_response_ids.unsqueeze(-1)\n", "    ).squeeze(-1))\n", "    log_opponent_probs_syn = torch.log(torch.gather(\n", "        opponent_probs_syn, dim=2, index=synthetic_response_ids.unsqueeze(-1)\n", "    ).squeeze(-1))\n\n", "    # Calculate log probability ratios for the tokens in the sequence\n", "    # [batch_size, sequence_length]\n", "    log_prob_ratio_gt = log_model_probs_gt - log_opponent_probs_gt\n", "    log_prob_ratio_syn = log_model_probs_syn - log_opponent_probs_syn\n\n", "    # Sum the log probability ratios over the sequence\n", "    # [batch_size] -> scalar\n", "    sum_log_prob_ratio_gt = torch.sum(log_prob_ratio_gt, dim=1)\n", "    sum_log_prob_ratio_syn = torch.sum(log_prob_ratio_syn, dim=1)\n\n", "    # Calculate the combined loss term for each sequence in the batch, scaled by lambda_reg\n", "    # [batch_size] -> scalar\n", "    combined_loss = lambda_reg * (sum_log_prob_ratio_gt - sum_log_prob_ratio_syn)\n\n", "    # Apply the logistic loss to the combined term\n", "    # [batch_size] -> scalar\n", "    logistic_loss = torch.log(1 + torch.exp(-combined_loss))\n\n", "    # Compute the mean of the logistic loss across the batch\n", "    # scalar\n", "    spin_loss = logistic_loss.mean()\n", "    return spin_loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, peft_model.parameters()), lr=5e-5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training loop for T iterations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["T = 5  # Set the number of iterations\n", "for iteration in range(T):\n", "    total_loss = 0\n", "    \n", "    # Disable adapter layers for the opponent model\n", "    peft_model.disable_adapter_layers()\n", "    \n", "    synthetic_data = []\n", "    opponent_logits_gt_list = []\n", "    for data in dataset:\n", "        prompt = data['prompt']\n", "        # Tokenize and generate synthetic data using the opponent model\n", "        prompt_encoding = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n", "        prompt_ids = prompt_encoding['input_ids']\n", "        prompt_attention_mask = prompt_encoding['attention_mask']\n", "        with torch.no_grad():\n", "            peft_model.eval()  # Set model to evaluation mode\n", "            #Generate synthetic responses using the opponent model\n", "            synthetic_response_ids = peft_model.generate(\n", "                prompt_ids, \n", "                attention_mask=prompt_attention_mask, \n", "                max_length=50\n", "            )\n", "            synthetic_data.append(synthetic_response_ids)\n", "            \n", "            # Calculate opponent's logits for ground truth responses\n", "            ground_truth = data['response']\n", "            ground_truth_encoding = tokenizer(\n", "                ground_truth, return_tensors='pt', padding=True, truncation=True\n", "            ).to(device)\n", "            ground_truth_ids = ground_truth_encoding['input_ids']\n", "            ground_truth_attention_mask = ground_truth_encoding['attention_mask']\n", "            opponent_logits_gt = peft_model(\n", "                input_ids=ground_truth_ids, \n", "                attention_mask=ground_truth_attention_mask\n", "            ).logits\n", "            opponent_logits_gt_list.append(opponent_logits_gt)\n\n", "    # Enable adapter layers for training the main player model\n", "    peft_model.enable_adapter_layers()\n", "    \n", "    # Train the main player model using the synthetic data and real responses\n", "    peft_model.train()  # Set model to training mode\n", "    for i, data in enumerate(dataset):\n", "        # Tokenize ground truth response for training\n", "        ground_truth_encoding = tokenizer(\n", "            data['response'], return_tensors='pt', padding=True, truncation=True\n", "        ).to(device)\n", "        ground_truth_ids = ground_truth_encoding['input_ids']\n", "        ground_truth_attention_mask = ground_truth_encoding['attention_mask']\n", "        synthetic_response_ids = synthetic_data[i].to(device)\n", "        opponent_logits_gt = opponent_logits_gt_list[i]\n\n", "        # Calculate logits for ground truth and synthetic responses using the main player model\n", "        main_player_logits_gt = peft_model(\n", "            input_ids=ground_truth_ids, \n", "            attention_mask=ground_truth_attention_mask\n", "        ).logits\n", "        main_player_logits_syn = peft_model(\n", "            input_ids=synthetic_response_ids\n", "        ).logits\n\n", "        # Compute logits for synthetic responses using the opponent model (disabled adapter layers)\n", "        peft_model.disable_adapter_layers()\n", "        opponent_logits_syn = peft_model(\n", "            input_ids=synthetic_response_ids\n", "        ).logits\n", "        peft_model.enable_adapter_layers()\n\n", "        # Compute the loss\n", "        loss = compute_spin_loss(\n", "            main_player_logits_gt, opponent_logits_gt,\n", "            main_player_logits_syn, opponent_logits_syn,\n", "            ground_truth_ids, synthetic_response_ids, lambda_reg\n", "        )\n", "        total_loss += loss.item()\n\n", "        # Backpropagation and optimization\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n\n", "    # Print average loss\n", "    average_loss = total_loss / len(dataset)\n", "    print(f\"Iteration {iteration + 1}/{T}, Average Loss: {average_loss}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save the final model parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["final_model_params = peft_model.state_dict()\n", "print(\"Training complete.\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}